# -*- coding: utf-8 -*-
"""EncoderDecoder NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11rxP239UXJvruFNuwL-Dcc9LztVPqbfi
"""

import os
import numpy as np
import zipfile
from urllib import request
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import random
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import cv2
from PIL import Image, ImageOps

import glob
input_files = glob.glob('./input/training_input/*.jpg')
ground_truth = glob.glob('./input/training_ground-truth/*.jpg')
print(len(input_files))
print(len(ground_truth))

input_img_list = []
gt_img_list = []
for img_file in input_files:
  image = Image.open(img_file)
  im2 = ImageOps.grayscale(image)
  im2.thumbnail((100,145), Image.ANTIALIAS)
  flatten_img = list(np.array(im2).flatten())
  input_img_list.append(flatten_img)
print(np.shape(np.array(input_img_list)))
input_array = np.array(input_img_list)
for img_file in ground_truth:
  image = Image.open(img_file)
  im2 = ImageOps.grayscale(image)
  im2.thumbnail((100,145), Image.ANTIALIAS)
  flatten_img = list(np.array(im2).flatten())
  gt_img_list.append(flatten_img)
print(np.shape(np.array(gt_img_list)))
gt_array = np.array(gt_img_list)

def get_random_img_list(batch_size):
  res = random.sample(range(0, 512), batch_size)
  x_batch = []
  y_batch = []
  for index in res:
    x_batch.append(input_array[index])
    y_batch.append(gt_array[index])
  return x_batch,y_batch

n_input = 14500
h_1 = 8192
h_2 = 4096
h_3 = 256
h_4 = 256
h_5 = 4096
h_6 = 8192
output_layer = 14500

learning_rate = 0.03
epochs = 30
batch_size = 64

X = tf.placeholder(tf.float32, [None,n_input])
Y = tf.placeholder(tf.float32, [None,output_layer])

Z1 = tf.layers.dense(X,h_1,activation=tf.nn.relu)
Z2 = tf.layers.dense(Z1,h_2,activation=tf.nn.relu)
Z3 = tf.layers.dense(Z2,h_3,activation=tf.nn.relu)
Z4 = tf.layers.dense(Z3,h_4,activation=tf.nn.relu)
Z5 = tf.layers.dense(Z4,h_5,activation=tf.nn.relu)
Z6 = tf.layers.dense(Z5,h_6,activation=tf.nn.relu)
NN_output = tf.layers.dense(Z6,output_layer)

computed_loss = tf.reduce_mean(tf.square(NN_output - Y))
optimizer = tf.train.AdamOptimizer().minimize(computed_loss)
init = tf.global_variables_initializer()

total_num_images = len(input_files)

sess = tf.Session()
sess.run(init)
for epoch in range(epochs):
  for i in range(int(total_num_images/batch_size)):
    #X_epoch = input_img_list[i*batch_size:(i+1)*batch_size]
    #Y_epoch = gt_img_list[i*batch_size:(i+1)*batch_size]
    X_epoch,Y_epoch = get_random_img_list(batch_size)
    acc ,loss = sess.run([optimizer, computed_loss],feed_dict = {X:X_epoch,Y:Y_epoch})
  print('Epoch: ', epoch,'/',epochs, '\t','loss: ',loss)

denoised_image = sess.run(NN_output,feed_dict={X:input_array[:10]})

#denoised_image

i = 0
#for img in denoised_image:
  #img2 = vec2matrix(img,ncol=100)
img = denoised_image[9]
img = img/255
img2 = np.reshape(img,(-1,100))
data = Image.fromarray(img2)
print(data.size)
data = data.convert("L")
data.save('h.jpg')
plt.imshow(data)


