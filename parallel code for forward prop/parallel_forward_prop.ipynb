{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "parallel_forward_prop.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivg9OZ8acZPZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01fa750d-7bc9-42d7-f265-3d79723690de"
      },
      "source": [
        "pip install pycuda"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycuda\n",
            "  Downloading pycuda-2021.1.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytools>=2011.2\n",
            "  Downloading pytools-2021.2.9.tar.gz (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pycuda) (1.4.4)\n",
            "Collecting mako\n",
            "  Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from pytools>=2011.2->pycuda) (1.19.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from mako->pycuda) (2.0.1)\n",
            "Building wheels for collected packages: pycuda, pytools\n",
            "  Building wheel for pycuda (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2021.1-cp37-cp37m-linux_x86_64.whl size=627871 sha256=e4eeec83829353b769247809795d51a7e1f5a2ae214d14c10c581d7262de36c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/ef/49/dc6a5feb8d980b37c83d465ecab24949a6aa19458522a9e001\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2021.2.9-py2.py3-none-any.whl size=62370 sha256=3a8ba28d439136d62f21ed9122251d5f6714b6878fbc735d607c4d4915a9475a\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/b9/6e/94bb014f6484b15ec77e7877f3a227609481ffd98db364504d\n",
            "Successfully built pycuda pytools\n",
            "Installing collected packages: pytools, mako, pycuda\n",
            "Successfully installed mako-1.1.6 pycuda-2021.1 pytools-2021.2.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiAIMorNdfrT"
      },
      "source": [
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "import numpy\n",
        "import time\n",
        "import math"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqJri1AFARz4"
      },
      "source": [
        "def formatting(a,b): #needed only once\n",
        "    try:\n",
        "        x = len(a[0][0][0])\n",
        "        for i in range(len(a)):\n",
        "            for j in range(len(a[0])):\n",
        "                for k in range(len(a[0][0])):\n",
        "                    for l in range(len(a[0][0][0])):\n",
        "                        b[l][k][i][j] = a[i][j][k][l]\n",
        "\n",
        "    except:\n",
        "        for i in range(len(a)):\n",
        "            for j in range(len(a[0])):\n",
        "                for k in range(len(a[0][0])):\n",
        "                    b[k][i][j] = a[i][j][k]\n",
        "    return b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4EoxvbtVMMe"
      },
      "source": [
        "def deformatting(X,Y): #needed only once\n",
        "    for i in range(len(X[0])):\n",
        "        for j in range(len(X[0][0])):\n",
        "            for k in range(len(X)):\n",
        "                Y[i][j][k] = X[k][i][j]\n",
        "    return Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYV5qYOKg9Yj"
      },
      "source": [
        "## Parallel sum of arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zaa9mNBQrfA8"
      },
      "source": [
        "def kernel_for_sum(arr1,arr2):\n",
        "\n",
        "    h = numpy.int32(len(arr1))\n",
        "    w = numpy.int32(len(arr1[0]))\n",
        "\n",
        "    grid_h = math.ceil(h/32)\n",
        "    grid_w = math.ceil(w/32)\n",
        "\n",
        "    arr1_d = cuda.mem_alloc(arr1.nbytes)\n",
        "    arr2_d = cuda.mem_alloc(arr2.nbytes)\n",
        "\n",
        "    cuda.memcpy_htod(arr1_d,arr1)\n",
        "    cuda.memcpy_htod(arr2_d,arr2)\n",
        "\n",
        "    mod = SourceModule(\"\"\"\n",
        "    __global__ void Sum_of_arrays(int *arr1_d, int *arr2_d, int w) {\n",
        "            int row = blockIdx.y*blockDim.y + threadIdx.y;\n",
        "            int col = blockIdx.x*blockDim.y + threadIdx.x;\n",
        "\n",
        "            arr1_d[row*w + col] = arr1_d[row*w + col] + arr2_d[row*w + col];\n",
        "        }\n",
        "    \"\"\")\n",
        "\n",
        "    Sum_of_arrays = mod.get_function(\"Sum_of_arrays\")\n",
        "    Sum_of_arrays(arr1_d,arr2_d,w,block=(32,32,1),grid=(grid_w,grid_h,1))\n",
        "\n",
        "    cuda.memcpy_dtoh(arr1,arr1_d)\n",
        "    return arr1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsBtlBAZUUP2"
      },
      "source": [
        "## Parallel Convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox2eYvO0S7Ew"
      },
      "source": [
        "def convolution_single_step(tempinput, tempfilter, tempoutput):\n",
        "\n",
        "    mod = SourceModule(\"\"\"\n",
        "__global__ void convolution_2d(int *input, int *filter, int *output, int height, int width){\n",
        "    int FILTER_DIM = 3;\n",
        "    int FILTER_OFFSET = FILTER_DIM/2 ;\n",
        "\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    int start_r = row - FILTER_OFFSET;\n",
        "    int start_c = col - FILTER_OFFSET;\n",
        "\n",
        "    int temp = 0;\n",
        "    for (int i = 0; i < FILTER_DIM; i++){\n",
        "        for (int j = 0; j < FILTER_DIM; j++){\n",
        "            if ((start_r + i) >= 0 && (start_r + i) < height){\n",
        "                if ((start_c + j) >= 0 && (start_c + j) < width){\n",
        "                    temp += input[(start_r + i) * width + (start_c + j)] * filter[i * FILTER_DIM + j];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    output[row * width + col] = temp;\n",
        "}\n",
        "\"\"\")\n",
        "    height = numpy.int32(h)\n",
        "    width = numpy.int32(w)\n",
        "    \n",
        "    gridHeight = math.ceil(h/32)\n",
        "    gridWidth = math.ceil(w/32)\n",
        "\n",
        "    #start = time.time()\n",
        "    conv = mod.get_function(\"convolution_2d\")\n",
        "\n",
        "    #diff = 0\n",
        "\n",
        "    tempinput = tempinput.astype(numpy.int32)\n",
        "    tempfilter = tempfilter.astype(numpy.int32)\n",
        "    tempoutput = tempoutput.astype(numpy.int32)\n",
        "\n",
        "    tempinputD = cuda.mem_alloc(tempinput.nbytes)\n",
        "    tempfilterD = cuda.mem_alloc(tempfilter.nbytes)\n",
        "    tempoutputD = cuda.mem_alloc(tempoutput.nbytes)\n",
        "\n",
        "    cuda.memcpy_htod(tempinputD, tempinput)\n",
        "    cuda.memcpy_htod(tempfilterD, tempfilter)\n",
        "    cuda.memcpy_htod(tempoutputD, tempoutput)\n",
        "\n",
        "    #startker = time.time()\n",
        "    conv(tempinputD, tempfilterD, tempoutputD, height, width, block=(4,4,1), grid=(gridWidth,gridHeight,1))  # block format is important! (width,height,1)\n",
        "    #endker = time.time()\n",
        "\n",
        "    #differencekernel = endker-startker\n",
        "    #diff += differencekernel\n",
        "\n",
        "    cuda.memcpy_dtoh(tempoutput, tempoutputD)\n",
        "\n",
        "    #end = time.time()\n",
        "\n",
        "    return tempoutput\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN_SJxP1QAIV"
      },
      "source": [
        "def convolution_layer(X,W):\n",
        "    (channels, height_X, width_X) = X.shape\n",
        "    \n",
        "    (filters, channels, height_W, width_W) = W.shape\n",
        "    \n",
        "    final_output = []\n",
        "    for f in range(filters):\n",
        "        base_array = numpy.zeros(shape=(height_X,width_X)).astype(numpy.int32)\n",
        "        final_output.append([])\n",
        "        for c in range(channels):   \n",
        "            tempoutput = numpy.zeros(shape=(height_X,width_X))\n",
        "            one_channel_output = convolution_single_step(X[c], W[f][c], tempoutput)   \n",
        "            one_kernel_output = kernel_for_sum(base_array,one_channel_output)            \n",
        "            base_array = one_kernel_output\n",
        "        final_output[-1] = one_kernel_output\n",
        "    return numpy.array(final_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYAlZd_HWojD",
        "outputId": "b3d156a4-29ab-4734-bdf2-4282bdcf7794"
      },
      "source": [
        "h = 400\n",
        "w = 288\n",
        "ch = 32\n",
        "filters = 1\n",
        "W_w = W_h = 3\n",
        "\n",
        "X = numpy.random.randint(1,5,size=(ch,h,w))\n",
        "X = X.astype(numpy.int32)\n",
        "\n",
        "W = numpy.random.randint(1,3,size=(filters,ch,W_h,W_w))\n",
        "W = W.astype(numpy.int32)\n",
        "\n",
        "\n",
        "#print(X)\n",
        "#print(W)\n",
        "timeStart = time.time()\n",
        "s = convolution_layer(X, W)\n",
        "timeEnd = time.time()\n",
        "#print(s)\n",
        "print(numpy.shape(s))\n",
        "print(timeEnd-timeStart)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 400, 288)\n",
            "0.10384821891784668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n-oo7rlvf7u"
      },
      "source": [
        "**PARALLEL MAXPOOLING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBZJsejfwAV4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRqdBTiZvmVi"
      },
      "source": [
        "def Max_Pool_Single_Step(tempinput,tempoutput):\n",
        "    widthO = w/2\n",
        "    heightO = h/2\n",
        "    widthO = numpy.int32(widthO)\n",
        "    heightO = numpy.int32(heightO)\n",
        "    mod = SourceModule(\"\"\"\n",
        "    __global__ void maxpool(int *input, int *output, int inputDimX, int inputDimY, int outputDimX, int outputDimY){\n",
        "\n",
        "        int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "        int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "        if((row<outputDimY)&&(col<outputDimX)){\n",
        "            int temp[4];\n",
        "            temp[0] = input[(2*row) * inputDimX + (2*col)];\n",
        "            temp[1] = input[(2*row) * inputDimX + (2*col+1)]; \n",
        "            temp[2] = input[(2*row+1) * inputDimX + (2*col)]; \n",
        "            temp[3] = input[(2*row+1) * inputDimX + (2*col+1)];\n",
        "\n",
        "            int i, max = 0;\n",
        "            for(i=0; i<4; i++){\n",
        "                if(temp[i]>max){\n",
        "                    max = temp[i];\n",
        "                }\n",
        "            }\n",
        "            output[row * outputDimX + col] = max;\n",
        "        }\n",
        "    }\n",
        "    \"\"\")\n",
        "    gridHeight = math.ceil(h/64)\n",
        "    gridWidth = math.ceil(w/64)\n",
        "\n",
        "    diff = 0\n",
        "\n",
        "    start = time.time()\n",
        "    Mpool = mod.get_function(\"maxpool\")\n",
        "\n",
        "\n",
        "    tempinput = tempinput.astype(numpy.int32)\n",
        "    tempoutput = tempoutput.astype(numpy.int32)\n",
        "\n",
        "    tempinputD = cuda.mem_alloc(tempinput.nbytes)\n",
        "    tempoutputD = cuda.mem_alloc(tempoutput.nbytes)\n",
        "\n",
        "    cuda.memcpy_htod(tempinputD, tempinput)\n",
        "    cuda.memcpy_htod(tempoutputD, tempoutput)\n",
        "\n",
        "    startker = time.time()\n",
        "    Mpool(tempinputD, tempoutputD, width, height, widthO, heightO, block=(32,32,1), grid=(gridWidth,gridHeight,1))  # block format is important! (width,height,1)\n",
        "    endker = time.time()\n",
        "    \n",
        "    differencekernel = endker-startker\n",
        "    diff += differencekernel\n",
        "\n",
        "    cuda.memcpy_dtoh(tempoutput, tempoutputD)\n",
        "    return tempoutput"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzD3koSPVPY1"
      },
      "source": [
        "def Max_pool_layer(X):\n",
        "    (channels,height,width) = X.shape\n",
        "    widthO = w/2\n",
        "    heightO = h/2\n",
        "    widthO = numpy.int32(widthO)\n",
        "    heightO = numpy.int32(heightO)\n",
        "    final_output = []\n",
        "    for c in range(channels):\n",
        "        output = numpy.zeros(shape=(heightO,widthO)).astype(numpy.int32)\n",
        "        one_channel_output = Max_Pool_Single_Step(X[c],output)\n",
        "        final_output.append(one_channel_output)\n",
        "    return numpy.array(final_output)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-vqGsUuXPgt",
        "outputId": "cdfbf7a1-41ee-4550-8e88-d78ff873d9df"
      },
      "source": [
        "h = 64\n",
        "w = 64\n",
        "c = 1\n",
        "\n",
        "\n",
        "X = numpy.random.randint(0,10,size=(c,h,w)).astype(numpy.int32)\n",
        "height = numpy.int32(h)\n",
        "width = numpy.int32(w)\n",
        "channel = numpy.int32(c)\n",
        "\n",
        "print(X)\n",
        "m = Max_pool_layer(X)\n",
        "print(\"----------\")\n",
        "print(m)\n",
        "print(numpy.shape(m))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[9 7 8 ... 5 0 5]\n",
            "  [6 2 1 ... 1 6 6]\n",
            "  [3 5 2 ... 9 2 8]\n",
            "  ...\n",
            "  [0 2 5 ... 2 7 5]\n",
            "  [6 1 1 ... 6 2 2]\n",
            "  [2 6 7 ... 5 0 4]]]\n",
            "----------\n",
            "[[[9 9 6 ... 8 5 6]\n",
            "  [9 5 9 ... 6 9 8]\n",
            "  [9 7 8 ... 9 9 6]\n",
            "  ...\n",
            "  [8 9 9 ... 1 9 8]\n",
            "  [4 8 8 ... 8 8 7]\n",
            "  [6 8 6 ... 7 6 4]]]\n",
            "(1, 32, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7lbVUsXfBxK"
      },
      "source": [
        "**CONVOLUTION TRANSPOSE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHVura8KfkoE"
      },
      "source": [
        "def conv2DTranspose(tempinput, tempfilter, tempoutput):\n",
        "    mod = SourceModule(\"\"\"\n",
        "    __global__ void convTranspose(int *input, int *filter, int *output, int inputDimX, int inputDimY, int outputDimX){\n",
        "\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    int i, j, temp[4];\n",
        "    for(i = 0; i < 2; i++){\n",
        "        for(j = 0; j < 2; j++){\n",
        "            if((row<inputDimY)&&(col<inputDimX)){\n",
        "                output[(2*row + i) * outputDimX + (2*col + j)] = input[row * inputDimX + col] * filter[2*i + j];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    //output[(2*row) * outputDimX + (2*col)] = temp[0];\n",
        "    //output[(2*row) * outputDimX + (2*col+1)] = temp[1];\n",
        "    //output[(2*row+1) * outputDimX + (2*col)] = temp[2];\n",
        "    //output[(2*row+1) * outputDimX + (2*col+1)] = temp[3];\n",
        "\n",
        "    }\n",
        "    # \"\"\")\n",
        "    gridHeight = math.ceil(h/32)\n",
        "    gridWidth = math.ceil(w/32)\n",
        "\n",
        "    wo = 2*w\n",
        "\n",
        "    height = numpy.int32(h)\n",
        "    width = numpy.int32(w)\n",
        "\n",
        "    widthO = numpy.int32(wo)\n",
        "\n",
        "    #diff = 0\n",
        "\n",
        "    #start = time.time()\n",
        "    convtrans = mod.get_function(\"convTranspose\")\n",
        "\n",
        "    tempinput = tempinput.astype(numpy.int32)\n",
        "    tempfilter = tempfilter.astype(numpy.int32)\n",
        "    tempoutput = tempoutput.astype(numpy.int32)\n",
        "\n",
        "    tempinputD = cuda.mem_alloc(tempinput.nbytes)\n",
        "    tempfilterD = cuda.mem_alloc(tempfilter.nbytes)\n",
        "    tempoutputD = cuda.mem_alloc(tempoutput.nbytes)\n",
        "\n",
        "    cuda.memcpy_htod(tempinputD, tempinput)\n",
        "    cuda.memcpy_htod(tempfilterD, tempfilter)\n",
        "    cuda.memcpy_htod(tempoutputD, tempoutput)\n",
        "\n",
        "    startker = time.time()\n",
        "    convtrans(tempinputD, tempfilterD, tempoutputD, width, height, widthO, block=(32,32,1), grid=(gridWidth,gridHeight,1))  # block format is important! (width,height,1)\n",
        "    endker = time.time()\n",
        "    \n",
        "    #differencekernel = endker-startker\n",
        "    #diff += differencekernel\n",
        "\n",
        "    cuda.memcpy_dtoh(tempoutput, tempoutputD)\n",
        "\n",
        "    #end = time.time()\n",
        "\n",
        "    return tempoutput\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YlA75BYuGI-"
      },
      "source": [
        "def Conv2DTranspose_layer(X, W):\n",
        "    global total_ch\n",
        "    global total_soa\n",
        "    (n_c_prev,height,width) = X.shape\n",
        "    (filters,n_c_prev,f,f) = W.shape\n",
        "    final_output = []\n",
        "    for f in range(filters):\n",
        "        base_array = numpy.zeros(shape=(height*2,width*2)).astype(numpy.int32)  #initializing with zero\n",
        "        final_output.append([])\n",
        "        for c in range(n_c_prev):\n",
        "            start = time.time()\n",
        "            tempoutput = numpy.zeros(shape=(height*2,width*2))\n",
        "            one_channel_output =  conv2DTranspose(X[c], W[f][c], tempoutput)\n",
        "            # print(\"input\",X[c])\n",
        "            # print(\"filter\",W[f][c])\n",
        "            # print(\"output\",one_channel_output)\n",
        "            end = time.time()\n",
        "            total_ch+=(end-start)\n",
        "            #print(np.array(one_channel_output).shape)\n",
        "            start = time.time()\n",
        "            # one_kernel_output = sum_of_arrays(base_array,one_channel_output)\n",
        "            one_kernel_output = kernel_for_sum(base_array,one_channel_output)\n",
        "            #print(\"channel output\", one_kernel_output)\n",
        "            end = time.time()\n",
        "            #print(\"Time for one iteration:\",end-start)\n",
        "            total_soa+=(end-start)\n",
        "            #print(np.array(one_kernel_output).shape)\n",
        "            base_array = one_kernel_output\n",
        "        final_output[-1] = one_kernel_output\n",
        "    return numpy.array(final_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRsRHGPouIub",
        "outputId": "7200f125-db64-48e6-bdee-ebc04ea6cadd"
      },
      "source": [
        "h = 200\n",
        "w = 144\n",
        "ch = 64\n",
        "filters = 32\n",
        "W_w = W_h = 2\n",
        "\n",
        "X = numpy.random.randint(1,5,size=(ch,h,w))\n",
        "X = X.astype(numpy.int32)\n",
        "\n",
        "W = numpy.random.randint(1,3,size=(filters,ch,W_h,W_w))\n",
        "W = W.astype(numpy.int32)\n",
        "\n",
        "#print(X)\n",
        "#print(W)\n",
        "total_ch = 0\n",
        "total_soa = 0 #time taken by sum_of_arrays\n",
        "s = Conv2DTranspose_layer(X, W)\n",
        "#print(s)\n",
        "print(numpy.shape(s))\n",
        "print(\"time for convtrans: \",total_ch)\n",
        "print(\"time for sum_of_array function: \",total_soa)\n",
        "print(total_ch + total_soa)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 400, 288)\n",
            "time for convtrans:  1.5371813774108887\n",
            "time for sum_of_array function:  2.993251085281372\n",
            "4.530432462692261\n"
          ]
        }
      ]
    }
  ]
}